<?xml version="1.0" encoding="UTF-8" ?>
<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<configuration>

  <property>
    <name>fs.s3a.access.key</name>
    <description>AWS access key ID. Omit for Role-based authentication.</description>
    <value>S3A_ACCESS_KEY</value>
  </property>

  <property>
    <name>fs.s3a.secret.key</name>
    <description>AWS secret key. Omit for Role-based authentication.</description>
    <value>S3A_SECRET_KEY</value>
  </property>

  <property>
    <name>fs.s3a.connection.maximum</name>
    <value>S3A_CONNECTION_MAXIMUM</value>
    <description>Controls the maximum number of simultaneous connections to S3.</description>
  </property>

  <property>
    <name>fs.s3a.server-side-encryption-algorithm</name>
    <value>AES256</value>
  </property>

  <!--property>
    <name>fs.s3a.connection.ssl.enabled</name>
    <value>true</value>
    <description>Enables or disables SSL connections to S3.</description>
  </property-->

  <property>
    <name>fs.s3a.endpoint</name>
    <description>AWS S3 endpoint to connect to. An up-to-date list is
        provided in the AWS Documentation: regions and endpoints. Without this
        property, the standard region (s3.amazonaws.com) is assumed.
      </description>
    <value>S3A_ENDPOINT</value>
  </property>

  <!--property>
    <name>fs.s3a.proxy.host</name>
    <description>Hostname of the (optional) proxy server for S3 connections.</description>
  </property-->

  <!--property>
    <name>fs.s3a.proxy.port</name>
    <description>Proxy server port. If this property is not set
        but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with
        the value of fs.s3a.connection.ssl.enabled).</description>
  </property-->

  <!--property>
    <name>fs.s3a.proxy.username</name>
    <description>Username for authenticating with proxy server.</description>
  </property-->

  <!--property>
    <name>fs.s3a.proxy.password</name>
    <description>Password for authenticating with proxy server.</description>
  </property-->

  <!--property>
    <name>fs.s3a.proxy.domain</name>
    <description>Domain for authenticating with proxy server.</description>
  </property-->

  <!--property>
    <name>fs.s3a.proxy.workstation</name>
    <description>Workstation for authenticating with proxy server.</description>
  </property-->

  <!--property>
    <name>fs.s3a.attempts.maximum</name>
    <value>20</value>
    <description>How many times we should retry commands on transient errors.</description>
  </property-->

  <!--property>
    <name>fs.s3a.connection.establish.timeout</name>
    <value>5000</value>
    <description>Socket connection setup timeout in milliseconds.</description>
  </property-->

  <!--property>
    <name>fs.s3a.connection.timeout</name>
    <value>200000</value>
    <description>Socket connection timeout in milliseconds.</description>
  </property-->

  <!--property>
    <name>fs.s3a.paging.maximum</name>
    <value>5000</value>
    <description>How many keys to request from S3 when doing
         directory listings at a time.</description>
  </property-->

  <!--property>
    <name>fs.s3a.threads.max</name>
    <value>10</value>
    <description> Maximum number of concurrent active (part)uploads,
      which each use a thread from the threadpool.</description>
  </property-->

  <!--property>
    <name>fs.s3a.threads.keepalivetime</name>
    <value>60</value>
    <description>Number of seconds a thread can be idle before being
        terminated.</description>
  </property-->

  <!--property>
    <name>fs.s3a.max.total.tasks</name>
    <value>5</value>
    <description>Number of (part)uploads allowed to the queue before
      blocking additional uploads.</description>
  </property-->

  <!--property>
    <name>fs.s3a.multipart.size</name>
    <value>104857600</value>
    <description>How big (in bytes) to split upload or copy operations up into.</description>
  </property-->

  <!--property>
    <name>fs.s3a.multipart.threshold</name>
    <value>2147483647</value>
    <description>Threshold before uploads or copies use parallel multipart operations.</description>
  </property-->

  <!--property>
    <name>fs.s3a.multiobjectdelete.enable</name>
    <value>false</value>
    <description>When enabled, multiple single-object delete requests are replaced by
        a single 'delete multiple objects'-request, reducing the number of requests.
        Beware: legacy S3-compatible object stores might not support this request.
      </description>
  </property-->

  <!--property>
    <name>fs.s3a.acl.default</name>
    <description>Set a canned ACL for newly created and copied objects. Value may be private,
         public-read, public-read-write, authenticated-read, log-delivery-write,
         bucket-owner-read, or bucket-owner-full-control.</description>
  </property-->

  <!--property>
    <name>fs.s3a.multipart.purge</name>
    <value>false</value>
    <description>True if you want to purge existing multipart uploads that may not have been
         completed/aborted correctly</description>
  </property-->

  <!--property>
    <name>fs.s3a.multipart.purge.age</name>
    <value>86400</value>
    <description>Minimum age in seconds of multipart uploads to purge</description>
  </property-->

  <!--property>
    <name>fs.s3a.signing-algorithm</name>
    <description>Override the default signing algorithm so legacy
        implementations can still be used</description>
  </property-->

  <!--property>
    <name>fs.s3a.buffer.dir</name>
    <value>${hadoop.tmp.dir}/s3a</value>
    <description>Comma separated list of directories that will be used to buffer file
        uploads to. No effect if fs.s3a.fast.upload is true.</description>
  </property-->

  <!--property>
    <name>fs.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
    <description>The implementation class of the S3A Filesystem</description>
  </property-->

  <!--property>
    <name>fs.AbstractFileSystem.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3A</value>
    <description>The implementation class of the S3A AbstractFileSystem.</description>
  </property-->

</configuration>

